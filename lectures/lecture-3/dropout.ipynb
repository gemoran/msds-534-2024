{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout details\n",
    "\n",
    "Last lecture, I said that during evaluation, the weights were multiplied by $(1-p)$ so that their expectation is the same as during training. That is, in training, we have\n",
    "\\begin{align*}\n",
    "E[w_{jk} \\times e_{jk}] &= 0 \\times p \\times w_{jk} + (1 - p) \\times w_{jk}, \\quad e_{jk}\\sim\\text{Bernoulli}(1-p) \\\\\n",
    "&= (1-p)w_{jk}\n",
    "\\end{align*}\n",
    "So to keep the expectation the same at test time, we need to multiply all the weights by $(1-p)$.\n",
    "\n",
    "Alternatively, we can scale weights during training (instead of during evaluation). This is what PyTorch does.\n",
    "The weights that are not dropped out are scaled by $1/(1-p)$. Why?\n",
    "The expecation is:\n",
    "\\begin{align*}\n",
    "E[w_{jk} \\times e_{jk} / (1-p)] &= 0 \\times p/(1-p)\\times w_{jk} + (1-p)/(1-p)\\times w_{jk} \\\\\n",
    "&= w_{jk}\n",
    "\\end{align*}\n",
    "\n",
    "Let's look at an illustration of this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output:\n",
      "tensor([[0., 2., 2., 2., 0., 2., 0., 2., 2., 2.],\n",
      "        [2., 0., 2., 2., 0., 0., 2., 0., 0., 0.],\n",
      "        [2., 0., 0., 2., 0., 0., 0., 2., 0., 2.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 2., 0., 2.],\n",
      "        [0., 0., 0., 0., 2., 0., 2., 0., 2., 2.]])\n",
      "\n",
      "Evaluation output:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create a dropout layer with 50% dropout rate\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "# Input tensor\n",
    "x = torch.ones(5, 10) # 5 samples, each of dimension 10\n",
    "\n",
    "# Training mode (default)\n",
    "output_train = dropout(x)\n",
    "print(\"Training output:\")\n",
    "print(output_train)\n",
    "# outputs are scaled by 1/(1-0.5) = 2\n",
    "\n",
    "# Evaluation mode\n",
    "dropout.eval()\n",
    "output_eval = dropout(x)\n",
    "print(\"\\nEvaluation output:\")\n",
    "print(output_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens when we use dropout during training. (We manually code dropout so we can visualize the masks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutVisualization(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(DropoutVisualization, self).__init__()\n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            self.mask = torch.bernoulli(torch.full_like(x, 1 - self.p)) / (1 - self.p)\n",
    "            return x * self.mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = DropoutVisualization(p=dropout_prob) # or nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "input_size = 5\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "dropout_prob = 0.5\n",
    "\n",
    "# Create the network and optimizer\n",
    "model = SimpleNet(input_size, hidden_size, output_size, dropout_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up our optimizer and compute the loss and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create a mini-batch of samples\n",
    "batch_size = 15\n",
    "x = torch.randn(batch_size, input_size)\n",
    "y = torch.randn(batch_size, output_size)\n",
    "\n",
    "# Forward pass\n",
    "model.train()  # Set the model to training mode\n",
    "output = model(x)\n",
    "\n",
    "# Compute loss\n",
    "loss = nn.MSELoss()(output, y)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see which weights were dropped out for which samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout masks for each sample in the batch:\n",
      "Sample 1:\n",
      "tensor([2., 2., 2., 2., 0., 0., 0., 0., 2., 2.])\n",
      "\n",
      "Sample 2:\n",
      "tensor([0., 2., 0., 2., 0., 2., 0., 0., 0., 0.])\n",
      "\n",
      "Sample 3:\n",
      "tensor([2., 0., 2., 0., 2., 2., 2., 0., 2., 0.])\n",
      "\n",
      "Sample 4:\n",
      "tensor([0., 0., 2., 2., 2., 2., 2., 2., 0., 0.])\n",
      "\n",
      "Sample 5:\n",
      "tensor([2., 0., 2., 0., 0., 0., 0., 2., 0., 2.])\n",
      "\n",
      "Sample 6:\n",
      "tensor([0., 2., 0., 0., 2., 2., 2., 0., 0., 2.])\n",
      "\n",
      "Sample 7:\n",
      "tensor([0., 2., 0., 0., 0., 2., 2., 2., 0., 0.])\n",
      "\n",
      "Sample 8:\n",
      "tensor([0., 0., 2., 2., 0., 2., 2., 2., 2., 0.])\n",
      "\n",
      "Sample 9:\n",
      "tensor([2., 2., 2., 0., 2., 0., 0., 0., 0., 2.])\n",
      "\n",
      "Sample 10:\n",
      "tensor([2., 0., 0., 0., 0., 2., 2., 2., 0., 2.])\n",
      "\n",
      "Sample 11:\n",
      "tensor([2., 0., 0., 2., 0., 2., 0., 0., 2., 0.])\n",
      "\n",
      "Sample 12:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 2., 2., 0.])\n",
      "\n",
      "Sample 13:\n",
      "tensor([2., 2., 2., 0., 0., 2., 2., 0., 0., 0.])\n",
      "\n",
      "Sample 14:\n",
      "tensor([2., 2., 2., 2., 2., 0., 0., 2., 2., 2.])\n",
      "\n",
      "Sample 15:\n",
      "tensor([0., 2., 0., 0., 0., 0., 0., 2., 2., 2.])\n",
      "\n",
      "Sample 1: 4 out of 10 units dropped\n",
      "Sample 2: 7 out of 10 units dropped\n",
      "Sample 3: 4 out of 10 units dropped\n",
      "Sample 4: 4 out of 10 units dropped\n",
      "Sample 5: 6 out of 10 units dropped\n",
      "Sample 6: 5 out of 10 units dropped\n",
      "Sample 7: 6 out of 10 units dropped\n",
      "Sample 8: 4 out of 10 units dropped\n",
      "Sample 9: 5 out of 10 units dropped\n",
      "Sample 10: 5 out of 10 units dropped\n",
      "Sample 11: 6 out of 10 units dropped\n",
      "Sample 12: 8 out of 10 units dropped\n",
      "Sample 13: 5 out of 10 units dropped\n",
      "Sample 14: 2 out of 10 units dropped\n",
      "Sample 15: 6 out of 10 units dropped\n"
     ]
    }
   ],
   "source": [
    "# Visualize dropout mask for each sample in the batch\n",
    "print(\"Dropout masks for each sample in the batch:\")\n",
    "dropout_mask = model.dropout.mask\n",
    "for i in range(batch_size):\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(dropout_mask[i])\n",
    "    print()\n",
    "\n",
    "# Count dropped units for each sample\n",
    "dropped_units = (dropout_mask == 0).sum(dim=1)\n",
    "for i in range(batch_size):\n",
    "    print(f\"Sample {i + 1}: {dropped_units[i].item()} out of {hidden_size} units dropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how this impacts the gradients: weights which were dropped out for all samples have zero gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 4., 4., 2., 6., 4., 4., 2., 2.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_mask.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients before optimization step:\n",
      "fc1.weight grad:\n",
      "tensor([[-0.0746, -0.0586, -0.0095, -0.0553, -0.0109],\n",
      "        [-0.1191, -0.1163, -0.0267, -0.4393,  0.3315],\n",
      "        [-0.0185,  0.0294, -0.0818,  0.0658, -0.1012],\n",
      "        [-0.0302, -0.0059, -0.0357, -0.0163, -0.0188],\n",
      "        [ 0.2595, -0.1405, -0.3040,  0.1821, -0.4055],\n",
      "        [-0.0009, -0.0383,  0.0364, -0.1232,  0.0276],\n",
      "        [-0.0552, -0.0530,  0.0269, -0.2311,  0.1850],\n",
      "        [-0.7471, -0.1551,  0.3146, -0.4045,  0.4915],\n",
      "        [ 0.0190, -0.0561,  0.0690,  0.0069,  0.1228],\n",
      "        [ 0.0482,  0.1049, -0.1045,  0.2355, -0.1809]])\n",
      "\n",
      "fc1.bias grad:\n",
      "tensor([ 0.0420,  0.2904,  0.0239, -0.0202, -0.0982, -0.0267,  0.0480,  0.1704,\n",
      "        -0.0929,  0.1276])\n",
      "\n",
      "fc2.weight grad:\n",
      "tensor([[-0.7781,  0.0268,  0.0181, -0.0936,  0.2109, -0.0328, -0.2389,  0.4150,\n",
      "         -0.5130,  0.0824]])\n",
      "\n",
      "fc2.bias grad:\n",
      "tensor([-1.0284])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print gradients before the optimization step\n",
    "print(\"Gradients before optimization step:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name} grad:\")\n",
    "        print(param.grad)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then update our parameters based on the gradients. We see that the weights which were dropped out for all samples were not updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weights before and after update:\n",
      "fc1.weight:\n",
      "Before: tensor([[-0.0434, -0.2658, -0.3721, -0.0767, -0.3105],\n",
      "        [ 0.0178,  0.1556, -0.3232,  0.1080,  0.3442],\n",
      "        [ 0.2958,  0.2077, -0.3135,  0.4426,  0.4466],\n",
      "        [-0.1365, -0.0235,  0.2594,  0.4355,  0.1102],\n",
      "        [ 0.0728, -0.0578,  0.2075, -0.2832, -0.1104],\n",
      "        [ 0.3351, -0.3084, -0.1341,  0.0132, -0.3680],\n",
      "        [ 0.3294,  0.0711,  0.3388, -0.3490, -0.3195],\n",
      "        [ 0.0751,  0.2023,  0.0721, -0.0433,  0.2243],\n",
      "        [ 0.1800,  0.3127, -0.3854, -0.3738, -0.3710],\n",
      "        [ 0.1973, -0.1108, -0.2598,  0.1036,  0.0622]])\n",
      "After:  tensor([[-0.0427, -0.2652, -0.3721, -0.0759, -0.3104],\n",
      "        [ 0.0189,  0.1558, -0.3228,  0.1126,  0.3411],\n",
      "        [ 0.2956,  0.2073, -0.3130,  0.4417,  0.4474],\n",
      "        [-0.1365, -0.0235,  0.2594,  0.4355,  0.1102],\n",
      "        [ 0.0697, -0.0562,  0.2099, -0.2851, -0.1059],\n",
      "        [ 0.3352, -0.3081, -0.1345,  0.0144, -0.3683],\n",
      "        [ 0.3300,  0.0715,  0.3385, -0.3465, -0.3212],\n",
      "        [ 0.0822,  0.2038,  0.0700, -0.0399,  0.2190],\n",
      "        [ 0.1798,  0.3131, -0.3855, -0.3742, -0.3721],\n",
      "        [ 0.1967, -0.1117, -0.2587,  0.1013,  0.0640]])\n",
      "\n",
      "fc2.weight:\n",
      "Before: tensor([[-0.0365, -0.2981,  0.1102,  0.0970, -0.2369, -0.1833, -0.1010,  0.2622,\n",
      "          0.1139,  0.2765]])\n",
      "After:  tensor([[-0.0302, -0.2979,  0.1087,  0.0970, -0.2381, -0.1830, -0.0981,  0.2564,\n",
      "          0.1185,  0.2757]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform optimization step\n",
    "optimizer.step()\n",
    "\n",
    "# Print weights before and after the update\n",
    "print(\"\\nWeights before and after update:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:  # Only print weights, not biases\n",
    "        print(f\"{name}:\")\n",
    "        print(\"Before:\", param.data)\n",
    "        print(\"After: \", param.data - 0.01 * param.grad.data)  # lr * grad\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives of fc1 Weights in Matrix Notation\n",
    "\n",
    "#### Notation (with dimensions):\n",
    "\n",
    "- $\\mathbf{W}_1$: Weight matrix of fc1 (dimensions: hidden_size × input_size)\n",
    "- $\\mathbf{b}_1$: Bias vector of fc1 (dimensions: hidden_size × 1)\n",
    "- $\\mathbf{W}_2$: Weight matrix of fc2 (dimensions: output_size × hidden_size)\n",
    "- $\\mathbf{b}_2$: Bias vector of fc2 (dimensions: output_size × 1)\n",
    "- $\\mathbf{x}$: Input vector (dimensions: input_size × 1)\n",
    "- $\\mathbf{y}$: Target output vector (dimensions: output_size × 1)\n",
    "- $\\mathbf{h}$: Output of the hidden layer (dimensions: hidden_size × 1)\n",
    "- $\\mathbf{\\hat{y}}$: Predicted output vector (dimensions: output_size × 1)\n",
    "- $L$: Loss function (scalar)\n",
    "- $f$: ReLU activation function\n",
    "\n",
    "#### Derivative Calculation:\n",
    "\n",
    "We want to compute $\\frac{\\partial L}{\\partial \\mathbf{W}_1}$. \n",
    "\n",
    "1. $\\frac{\\partial L}{\\partial \\mathbf{\\hat{y}}} = (\\mathbf{\\hat{y}} - \\mathbf{y})^T$ (dimensions: 1 × output_size)\n",
    "\n",
    "2. $\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{h}} = \\mathbf{W}_2$ (dimensions: output_size × hidden_size)\n",
    "\n",
    "3. $\\frac{\\partial \\mathbf{h}}{\\partial (\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1)} = \\text{diag}(f'(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1))$ \n",
    "   This is a diagonal matrix $\\mathbf{F}'$ (dimensions: hidden_size × hidden_size)\n",
    "\n",
    "4. $\\frac{\\partial (\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1)}{\\partial \\mathbf{W}_1} = \\mathbf{x}^T$ \n",
    "   This is a tensor product, not a simple matrix multiplication.\n",
    "\n",
    "Now, let's combine these:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\mathbf{W}_1} = ((\\mathbf{\\hat{y}} - \\mathbf{y})^T \\mathbf{W}_2 \\mathbf{F}') \\otimes \\mathbf{x}^T$\n",
    "\n",
    "Where $\\otimes$ denotes the outer product (https://en.wikipedia.org/wiki/Outer_product)\n",
    "\n",
    "Let's break down the dimensions:\n",
    "\n",
    "- $(\\mathbf{\\hat{y}} - \\mathbf{y})^T$: 1 × output_size\n",
    "- $\\mathbf{W}_2$: output_size × hidden_size\n",
    "- $\\mathbf{F}'$: hidden_size × hidden_size\n",
    "- Result of $(\\mathbf{\\hat{y}} - \\mathbf{y})^T \\mathbf{W}_2 \\mathbf{F}'$: 1 x hidden_size\n",
    "- $\\mathbf{x}^T$: 1 x input_size \n",
    "\n",
    "The outer product of (1 × hidden_size) and (1 × input_size) results in a matrix of size (hidden_size × input_size), which correctly matches the dimensions of $\\mathbf{W}_1$.\n",
    "\n",
    "#### Interpretation:\n",
    "\n",
    "- $(\\mathbf{\\hat{y}} - \\mathbf{y})^T \\mathbf{W}_2$ computes how the error at the output layer affects the hidden layer.\n",
    "- Multiplying by $\\mathbf{F}'$ applies the ReLU derivative, effectively letting gradients flow only through active neurons.\n",
    "- The outer product with $\\mathbf{x}^T$ distributes this gradient to each weight based on the corresponding input value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msds534",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
